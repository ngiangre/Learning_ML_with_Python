{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagged Decision Trees for Classification\n",
      "\n",
      "0.770745044429\n",
      "\n",
      "Random Forest Classification\n",
      "\n",
      "0.768233082707\n",
      "\n",
      "Extra Trees\n",
      "\n",
      "0.76035543404\n",
      "\n",
      "AdaBoost\n",
      "\n",
      "0.76045796309\n",
      "\n",
      "Stochastic Gradient Boosting\n",
      "\n",
      "0.766900205058\n",
      "\n",
      "Voting Ensemble\n",
      "\n",
      "0.734295967191\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Combing Model Predictions into Ensembl Predictions\n",
    "#\n",
    "# http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "########################################\n",
    "\n",
    "# The three most popular methods for combining \n",
    "#the predictions from different models are:\n",
    "\n",
    "# Bagging: Building multiple models (typically \n",
    "#of the same type) from different subsamples of the training dataset.\n",
    "#\n",
    "# Boosting: Building multiple models (typically of \n",
    "#the same type) each of which learns to fix the prediction \n",
    "#errors of a prior model in the chain.\n",
    "#\n",
    "# Voting: Building multiple models (typically of \n",
    "#differing types) and simple statistics (like \n",
    "#calculating the mean) are used to combine predictions.\n",
    "\n",
    "\n",
    "# Bagging Algorithms\n",
    "\n",
    "# Bootstrap Aggregation or bagging \n",
    "#involves taking multiple samples from your \n",
    "#training dataset (with replacement) and \n",
    "#training a model for each sample\n",
    "\n",
    "# 1. Bagged Decision Tree\n",
    "\n",
    "# Bagging performs best with algorithms \n",
    "# that have high variance. A popular example \n",
    "# are decision trees, often constructed without pruning.\n",
    "\n",
    "# Bagged Decision Trees for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nBagged Decision Trees for Classification\\n\")\n",
    "print(results.mean())\n",
    "\n",
    "# 2. Random Forest\n",
    "\n",
    "# Random forest is an extension of bagged decision trees.\n",
    "\n",
    "# Samples of the training dataset are taken with \n",
    "# replacement, but the trees are constructed in a \n",
    "# way that reduces the correlation between individual \n",
    "# classifiers. Specifically, rather than greedily \n",
    "# choosing the best split point in the construction \n",
    "# of the tree, only a random subset of features are \n",
    "# considered for each split.\n",
    "\n",
    "# Random Forest Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nRandom Forest Classification\\n\")\n",
    "print(results.mean())\n",
    "\n",
    "# 3. Extra Trees\n",
    "\n",
    "# Extra Trees are another modification of \n",
    "# bagging where random trees are constructed \n",
    "# from samples of the training dataset.\n",
    "\n",
    "# Extra Trees Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nExtra Trees\\n\")\n",
    "print(results.mean())\n",
    "\n",
    "# Boosting Algorithms\n",
    "\n",
    "# Boosting ensemble algorithms creates a sequence \n",
    "# of models that attempt to correct the mistakes \n",
    "# of the models before them in the sequence.\n",
    "\n",
    "# Once created, the models make predictions \n",
    "# which may be weighted by their demonstrated \n",
    "# accuracy and the results are combined to create a final output prediction.\n",
    "\n",
    "# The two most common methods are:\n",
    "\n",
    "\n",
    "# 1. AdaBoost\n",
    "\n",
    "# AdaBoost was perhaps the first successful \n",
    "# boosting ensemble algorithm. It generally \n",
    "# works by weighting instances in the dataset \n",
    "# by how easy or difficult they are to classify, \n",
    "# allowing the algorithm to pay more or less attention \n",
    "# to them in the construction of subsequent models.\n",
    "\n",
    "# AdaBoost Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nAdaBoost\\n\")\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "# 2. Stochastic Gradient Boosting\n",
    "\n",
    "# Stochastic Gradient Boosting (also called Gradient \n",
    "# Boosting Machines) are one of the most sophisticated \n",
    "# ensemble techniques. It is also a technique that \n",
    "# is proving to be perhaps of the the best techniques \n",
    "# available for improving performance via ensembles.\n",
    "\n",
    "# Stochastic Gradient Boosting Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"\\nStochastic Gradient Boosting\\n\")\n",
    "print(results.mean())\n",
    "\n",
    "# Voting Ensemble\n",
    "\n",
    "#weighting predictions from sub-models\n",
    "\n",
    "# Voting Ensemble for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(\"\\nVoting Ensemble\\n\")\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
