{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Univariate Selection\n",
      "\n",
      "[  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393\n",
      "   181.304]\n",
      "[[ 148.     0.    33.6   50. ]\n",
      " [  85.     0.    26.6   31. ]\n",
      " [ 183.     0.    23.3   32. ]\n",
      " [  89.    94.    28.1   21. ]\n",
      " [ 137.   168.    43.1   33. ]]\n",
      "\n",
      "2. Recursive Feature Elimination\n",
      "\n",
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n",
      "\n",
      "3. Principal Component Analysis\n",
      "\n",
      "Explained Variance: [ 0.889  0.062  0.026]\n",
      "[[ -2.022e-03   9.781e-02   1.609e-02   6.076e-02   9.931e-01   1.401e-02\n",
      "    5.372e-04  -3.565e-03]\n",
      " [ -2.265e-02  -9.722e-01  -1.419e-01   5.786e-02   9.463e-02  -4.697e-02\n",
      "   -8.168e-04  -1.402e-01]\n",
      " [ -2.246e-02   1.434e-01  -9.225e-01  -3.070e-01   2.098e-02  -1.324e-01\n",
      "   -6.400e-04  -1.255e-01]]\n",
      "\n",
      "4. Feature Importance\n",
      "\n",
      "[ 0.115  0.236  0.105  0.077  0.08   0.134  0.112  0.141]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "#Feature Selection Tutorial\n",
    "#\n",
    "#http://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "########################################\n",
    "\n",
    "#Picking features to increase our prediction power\n",
    "\n",
    "########################################\n",
    "### Three benefits of performing feature selection before \n",
    "### modeling your data are:\n",
    "\n",
    "# Reduces Overfitting: Less redundant data means less \n",
    "#opportunity to make decisions based on noise.\n",
    "\n",
    "# Improves Accuracy: Less misleading data means modeling \n",
    "#accuracy improves.\n",
    "\n",
    "# Reduces Training Time: Less data means that algorithms \n",
    "#train faster.\n",
    "\n",
    "########################################\n",
    "# 0. Loading and preparing data for feature selection\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# load data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1. Univariate Selection\n",
    "# Statistical tests can be used to select those features \n",
    "# that have the strongest relationship with the output variable.\n",
    "\n",
    "#The scikit-learn library provides the SelectKBest class that \n",
    "#can be used with a suite of different statistical tests to \n",
    "#select a specific number of features.\n",
    "\n",
    "#The example below uses the chi squared (chi^2) statistical test \n",
    "#for non-negative features to select 4 of the best features from \n",
    "#the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "# Questions: Why Chi-squared? Why k=4?\n",
    "print(\"\\n1. Univariate Selection\\n\")\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])\n",
    "\n",
    "########################################\n",
    "# 2. Recursive Feature Elimination\n",
    "\n",
    "# The Recursive Feature Elimination (or RFE) works \n",
    "#by recursively removing attributes and building a \n",
    "#model on those attributes that remain.\n",
    "\n",
    "# It uses the model accuracy to identify which attributes \n",
    "# (and combination of attributes) contribute the most to \n",
    "# predicting the target attribute.\n",
    "\n",
    "#The example below uses RFE with the logistic regression \n",
    "#algorithm to select the top 3 features. The choice of algorithm \n",
    "#does not matter too much as long as it is skillful and consistent.\n",
    "# Questions: Why just top 3?\n",
    "print(\"\\n2. Recursive Feature Elimination\\n\")\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "\n",
    "########################################\n",
    "# 3. Principal Component Analysis\n",
    "\n",
    "#Principal Component Analysis (or PCA) uses \n",
    "#linear algebra to transform the dataset into \n",
    "#a compressed form.\n",
    "\n",
    "#Generally this is called a data reduction technique. \n",
    "#A property of PCA is that you can choose the number \n",
    "#of dimensions or principal component in the transformed result.\n",
    "\n",
    "#In the example below, we use PCA and select 3 principal components.\n",
    "\n",
    "print(\"\\n3. Principal Component Analysis\\n\")\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)\n",
    "\n",
    "########################################\n",
    "# 4. Feature Importance\n",
    "\n",
    "# Bagged decision trees like Random Forest and \n",
    "# Extra Trees can be used to estimate the importance of features.\n",
    "\n",
    "# You can see that we are given an importance score for \n",
    "# each attribute where the larger score the more important the attribute.\n",
    "\n",
    "print(\"\\n4. Feature Importance\\n\")\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
